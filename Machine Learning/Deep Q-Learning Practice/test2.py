#!/usr/bin/env python3# -*- coding: utf-8 -*-"""Created on Sep 7 13:30:17 2023@author: ijeong-yeon"""import numpy as npimport randomimport osimport torchimport torch.nn as nnimport torch.nn.functional as F import torch.optim as optimimport torch.autograd as autofrom torch.autograd import Variableclass Network(nn.Module):                # 객체 이름, Input Layer(매개/독립 변수들), output Layer 사이즈    def __init__(self, input_size, nb_action):        super(Network, self).__init__() #nn.Module의 모든 기능을 사용 가능하게 만들어줌        self.input_size = input_size        self.nb_action = nb_action        # Full connection between all the layers in AI        self.fc1 = nn.Linear(input_size, 30) # Input Layer + Hidden Layers 숫자 지정        self.fc2 = nn.Linear(30, nb_action)            def forward(self, state):        x = F.relu(self.fc1(state)) # Activate hidden neuron        q_values = self.fc2(x)        return q_values    class Replay_Memory(object):                # 객체 이름, 메모리 용량    def __init__(self, capacity):        self.capacity = capacity        self.memory = []            def push(self, event):        self.memory.append(event)        if len(self.memory) > self.capacity:            del self.memory[0]        def sample(self, batch_size):        samples = zip(*random.sample(self.memory, batch_size))        # random.sample을 통해 무작위로 함수를 memory에서 batch_size만큼 가져옴        # zio()을 통해 상태, 행동, 보상의 형태로 리스트 형태를 변경 -> 이를 파이토치에 적용해 경사를 구할 수 있음        return map(lambda x : Variable(torch.cat(x, 0)), samples)        # 샘플을 파이토치 텐서로 변환후 이를 경사가 포함된 변수로 재변환        class Dqn():        def __init__(self, input_size, nb_action, gamma):        self.gamma = gamma        self.reward_window = []        self.model = Network(input_size, nb_action)        self.memory = Replay_Memory(100000)        self.optimizer = optim.Adam(self.model.parameters(), lr = 0.001)        self.last_state = torch.Tensor(input_size).unsqueeze(0)        # 배치와 연동되는 가짜 성탸 차원 형성 (0에 형성) + 신경망의 차원        self.last_action = 0        self.last_reward = 0            def select_action(self, state):        probs = F.softmax(self.model(Variable(state, volatile = True))*7)        # volatile = True를 통해 경사를 변수에서 제외한 후 텐서를 만        # 온도 변수 *n: 0에 가까울수록 모델의 알고리즘에 대한 확신도가 줄어듦        action = probs.multinomial(num_samples=1) #확률 분산에서의 무작위 추첨 (가짜 토치 텐서를 반환)        return action.data[0,0]            # 인덱스: 상태 = 0, 행동 = 1, 보상 = 2    def learn(self, batch_state, batch_next_state, batch_reward, batch_action):        outputs = self.model(batch_state).gather(1, batch_action.unsqueeze(1)).squeeze(1)        next_outputs = self.model(batch_next_state).detach().max(1)[0]        target = (self.gamma * next_outputs) + batch_reward        td_loss = F.smooth_l1_loss(outputs, target)        self.optimizer.zero_grad()        td_loss.backward(retain_variables = True)        self.optimizer.step()            def update(self, reward, new_signal):        new_state = torch.Tensor(new_signal).float().unsqueeze(0)        self.memory.push((self.last_state, new_state, torch.LongTensor([int(self.last_action)]), torch.Tensor([self.last_reward])))        # 메모리에는 이전 상태, 현 상태, 이전 행동, 이전 보상을 저장함        action2 = self.select_action(new_state)        if len(self.memory.memory) > 100:            batch_state, batch_next_state, batch_reward, batch_action = self.memory.sample(100)            self.learn(batch_state, batch_next_state, batch_reward, batch_action)        self.last_action = action2        self.last_state = new_state        self.last_reward = reward                           self.reward_window.append(reward)        if len(self.reward_window) > 1000:            del self.reward_window[0]        return action2        def score(self):        return sum(self.reward_window)/(len(self.reward_window) + 1)        def save(self):        torch.save({'state_dict' : self.model.state_dict(),                    'optimizer' : self.optimizer.state_dict()},                   'last_brain.pth')            def load(self):        if os.path.isfile('last_brain.pth') == True:            print('=> loading checkpoint...')            checkpoint = torch.load('last_brain.pth')            self.model.load_state_dict(checkpoint['state_dict'])            self.optimizer.load_state_dict(checkpoint['optimizer'])            print('Done!')        else:            print('No checkpoint found')